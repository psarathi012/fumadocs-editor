---
title: Multiple Prompts
description: Prompt Experiment Playground allows you to experiment with prompts in two ways 
---
1. Single Prompt with Multiple Models
2. Multiple Prompts with a Single Model
# Single Prompt with Multiple Models
Configure the Prompts:

1. To create a Prompt Experiment, click on the "+" icon located on the left sidebar.

2. Select a prompt already created in <a style={{color:"#FFA500"}} href="">`Single Prompt`</a>

<img src="./Multiple_prompts/img1.png" alt="chameleon" />
<img src="./Multiple_prompts/img2.png" alt="chameleon" />


3. Select a Model from the dropdown
4. Click the "+" icon to add more models shown below
 
<img src="./Multiple_prompts/img3.png" alt="chameleon" />

Finally after configuration it looks like this we have selected `GPT-4`,`Haiku` and `GPT 4o` for a single Prompt `(Entity Recognition version 11)` to keep the Prompt exact same.
<img src="./Multiple_prompts/img4.png" alt="chameleon" />

In this experiment, we aim to recognize entities from a letter provided as text context, as discussed in the Single Prompt. The system prompt is structured as follows: 
<Callout title="System Prompt" type="info">
  `Identify all entities
For the purpose of this task an entity is defined as a "person", "organization", or "location".
Limit your response to specific entities - ignore vague responses like "employees".
Display the results as JSON. The key will be the entity type that corresponds to a list of entities values.
and add locations too
refer to this {context} for answering`
</Callout>

In the system prompt the Model is instructed to return the output in a JSON form.
After putting the user prompt as
<Callout title="User Prompt" type="info">
  `List down all the entities`
</Callout>

<img src="./Multiple_prompts/img5.png" alt="chameleon" />

From all three models, we receive outputs along with metrics such as `latency`, `cost`, and `token counts` for each model, providing valuable information for assessment. In the analysis, it's observed that `Haiku` model missed several organization names and incorrectly categorized `"Ferrovial"` as a person, while it's an organization. On the other hand, `GPT-4` and `GPT-4o` models overlooked person names. These observations enable informed judgments regarding the performance and accuracy of each model.

# Multiple Prompt with Single Models
<img src="./Multiple_prompts/img6.png" alt="chameleon" />

In a similar experiment, we opt for `GPT-4` as the model for all three prompts. However, we select different versions of the `Entity Recognition Prompt (V-9, V-10, V-11)` and conduct the same experiment as described above. This allows us to evaluate and compare which prompt version yields the best results in terms of entity recognition accuracy.

<Callout title="Note" type="info">
  `Keep in mind that you can choose a maximum of five different models or five different prompts for comparison at a time.`
</Callout>


<Cards>
  <Card title="Prompt" href="https://nextjs.org/docs" />
  <Card title="Single Prompt" href="https://fumadocs.vercel.app" />
  <Card title="Prompt Chains" href="https://fumadocs.vercel.app" />

</Cards>
