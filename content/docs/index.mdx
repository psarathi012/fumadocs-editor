---
title: Single prompt
description: The Single Prompt feature in Maxim lets users experiment with their system and user prompts. This feature allows you to iterate over prompts and test their usefulness using the prompt IDE and test runs. It helps ensure prompts work well before integrating them into more complex workflows for your application.
---

## Create a prompt

To create a new Prompt:

- Navigate to the "Prompt" tab on the left side panel
- Click on “Prompt” and then Choose "Single Prompt"
- Click the "+" icon.
- You can choose to create a new prompt directly or create a folder and have prompts within that to better organise your prompts.
- A dialog box will then appear, allowing you to name your new prompt. You can also assign it to an existing folder.

<img src="./Evaluate/Prompts/prompts-single/img1.png" alt="single prompt" />

<img src="./Evaluate/Prompts/prompts-single/img2.png" alt="single prompt" />

## Select a model 

<img src="./Evaluate/Prompts/prompts-single/img3.png" alt="single prompt" />

The platform supports all open-source and closed models, including your own custom models, thereby enabling you to experiment seamlessly across any model(s)

## Add system prompt and user prompt

Add your system and user prompts that you want to experiment with.

<img src="./Evaluate/Prompts/prompts-single/img4.png" alt="single prompt" />

## Configure parameters

<img src="./Evaluate/Prompts/prompts-single/img5.png" alt="single prompt" />

You can also configure model parameters such as temperature, max tokens, and logit bias. Additionally, you can include prompt tools for function calls, and add custom text to stop the model from generating. 

Also, there are some Maxim-specific configurations that your can add like Tags. 

### Tags

<LoomVideo url="https://drive.google.com/file/d/1br_lRPsHfHgpg37RLk7lDLTdS3WiOj-Z/preview" />

You can use tags to make prompts uniquely identifiable. These tags can then be utilised by teams using the Maxim SDK to reference their SDK

```js title="config.js"
test("fetch prompts using tags", async () => {
	const prompt = await maxim.getPrompt(
		promptId,
		new QueryBuilder()
			.and()
			.deploymentVar("Environment", "test")
			.tag("CustomerId", 1234)
			.tag("grade", "A")
			.tag("test", true)
			.exactMatch()
			.build(),
	);
	expect(prompt.promptId).toBe(promptId);
	expect(prompt.versionId).toBe(config.dev.testAndTagsCustomerIdGradeAndTest);
	expect(prompt.version).toBe(5);
});
```
For example, in the code block above, the tags CustomerId=1234, grade=A, and test=true are assigned to a specific prompt. These tags make the prompt easily identifiable and can be used for targeted tasks

## Bring your variables to the prompt playground

We understand that prompts usually have some dynamic variables. We enable you to bring those dynamic variable to prompts by using ``` {<variable>} ``` i.e. variable name wrapped in curly braces. Watch the video attached to understand how or where this might be useful.

<LoomVideo url="https://drive.google.com/file/d/188ZAPq3Kzmgz2lYcGBlivjgDn05eoTTF/preview" />

## Bring your context to the prompt playground
We also allow you to attach your rag pipeline to your prompts which you can reference using the ``` {context} ``` variable in the prompts. If you have the ``` {context} ``` in the prompts it will first check if you have attached Context Sources or not. If you have Context Sources Attached then we replace then ``` {context} ``` with the retrieved context from Context Sources otherwise we look context in your playground/dataset depending on whether you are running the prompt on playground or running tests. 

<LoomVideo url="https://drive.google.com/file/d/1th1IjzmEqCwNfF-qR4Q-Pbl1xaki8u4W/preview" />

## Version your prompts
<img src="./Evaluate/Prompts/prompts-single/img6.png" alt="single prompt" />

On the platform, you can save different versions of your prompts along with custom description, allowing you to track changes, review and revert to previous iterations, and compare different versions. This also captures the authors and the created timestamp, so that you have complete visibility on the prompt changes. It also allows for robust experimentation across different versions. Another important aspect of versioning is that users can version both the system and user prompts according to their specific requirements.

<img src="./Evaluate/Prompts/prompts-single/img7.png" alt="single prompt" />

## Deploy your prompts with no code changes

The Prompt deployment workflow is designed for flexibility in managing how prompts are used across different environments and conditions. This feature enables having access to one click prompt deployment without any code changes. Users with appropriate access can deploy different versions of prompts tailored to specific feature flags such as the environment (e.g., production or staging) and tenant Id. These feature flags are referenced in Maxim as deployment variables. Once deployed the prompts with their deployment variables can be accessed using the Maxim SDK. This granular control allows for targeted prompt deployment, ensuring that the right version is used for the right environments. Setting up these rules is straightforward: pick a version, click Deploy, set the right deployment variables and rules, and deploy. For example, a user can specify that a certain prompt version is to be deployed only in the staging environment or only to users with a specific tenant_id. 

<LoomVideo url="https://drive.google.com/file/d/1huMfI4kkT6louIDIwatXsykvnwxQfAQB/preview" />

## Testing a prompt
To test and evaluate your prompt on different evaluators, follow the steps in Testing a Prompt<a style={{color:"#FFA500"}} href=""><code>Testing a Prompt.</code></a>


<Cards>
  <Card title="Overview" href="https://nextjs.org/docs" />
  <Card title="Multiple prompts" href="https://fumadocs.vercel.app" />
 <Card title="Prompt chain" href="https://fumadocs.vercel.app" />

</Cards>