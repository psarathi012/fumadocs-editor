---
title: Custom evaluators
---

In Maxim, you can create the following custom evaluators:

- API-based evaluator
- AI evaluator
- Human evaluator
- Programmatic evaluator

# Create a API based evaluator

To demonstrate this feature, we have developed a demo model for sentiment analysis. 

The model assigns a score to a given text where a negative score indicates negative sentiment, a positive score indicates positive sentiment and a score of 0 indicates a balanced sentiment. For example:

- "I hate you" has a score of -0.81, indicating a negative sentiment.
- "I am so happy to see this rainbow" has a score of 0.51, indicating a positive sentiment.

In the following steps, we will integrate this model into Maxim using the API evaluator.

```py
import nltk
from textblob import TextBlob

# TextBlob for sentiment analysis
def analyze_sentiment(text):
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    return polarity
```

In the above code, we use Textblob which is a library built on top of NLTK and Pattern and measures polarity as part of its sentiment analysis capabilities. Polarity indicates the sentiment expressed in a text, ranging from -1 to 1, where:

- **-1** represents a very negative sentiment,
- **0** represents a neutral sentiment,
- **+1** represents a very positive sentiment.

TextBlob analyzes the text by breaking it down into individual words and phrases, then evaluates the sentiment based on a predefined lexicon where words are scored for their positive or negative sentiment. The overall polarity score is the average of the individual scores of words and phrases in the text.

```py
from flask import Flask
from flask import request
app = Flask(__name__)

@app.route('/model',methods=['POST'])
def sentiment():
    body = request.get_json()
    print(body)
    output = analyze_sentiment(body['query'])
    response = {'response':output}
    return response
    

# main driver function
if __name__ == '__main__':

    # run() method of Flask class runs the application 
    # on the local development server.
    app.run(port=8000)
```
We then created a Flask app and exposed it to the public via an API using Ngrok. This can now be utilized on the Maxim platform for evaluation through the API evaluator.

To create an API-based evaluator you will have to follow these steps :

- Select "Evaluators" from the left panel.
- Click on the "+" icon.
- Choose "API-based".
- A UI will open where you can enter your API endpoint.

<img src="./Library/Evaluators/Custom_evaluators/img1.png" alt="http_workflow" />

Then you will need to:

- Enter the API endpoint as done in the workflow.
- In the Body, map the query to the output of your application.

<img src="./Library/Evaluators/Custom_evaluators/img2.png" alt="http_workflow" />

# Select grading criteria

- Map the score to the endpoint output.
- Optionally, map the reasoning.
- Select the pass criteria for each query and for every run.
- Finally, save the evaluator to use it in the workflow.

<img src="./Library/Evaluators/Custom_evaluators/img3.png" alt="http_workflow" />

As you can see in the report, when each output is sent to the API evaluator, it returns a polarity score. A score greater than 0 indicates positive sentiment, while a score less than 0 indicates negative sentiment.

# Create a human evaluator

There are a few simple steps to get started:

- Create a human evaluator by specifying the dimension you want to test (e.g., bias) and guidance for human raters Select the grading criteria for human raters to evaluate on. It could be binary 0-1 or on a scale of 1-5
- During the test-run, choose the human raters you want to assign the task to by specifying the email addresses
- Once the test run is complete, human raters get notified via email for an evaluation request
- Human raters provide their ratings based on the pre-specified dimensions and criteria. They can also add additional comments against their reviews
- The ratings and comments by the human evaluators are available in your test runs

# Create AI evaluator

<img src="./Library/Evaluators/Custom_evaluators/img4.png" alt="http_workflow" />

You can choose a model to be used as Judge and define the prompt to specify how the model should evaluate. Afterward, you can select grading criteria to configure an AI evaluator.

# Create a Programmatic Evaluator
<img src="./Library/Evaluators/Custom_evaluators/img5.png" alt="http_workflow" />

You can add code snippets which can then be used as an evaluator. For example the above shown function named <code>validate</code> takes three parameters: <code>input</code>, <code>output</code>, and <code>expectedOutput</code>. It aims to check if the output contains any special characters (such as !@#$%^&*(),.?":{}|\<>). It uses a regular expression (<code>specialCharRegex</code>) to match any of these special characters against the <code>output</code>. If any special character is found in the <code>output</code>, the function returns <code>true</code>, indicating that the <code>output</code> is considered valid.

- Programmatic evaluators, akin to other evaluators , allow defining pass/fail criteria.
- They enable direct testing of functions.
- For instance, if "\{run\}" is passed as output to the <code>validate</code> function, it returns true due to special characters "\{" and "\}".
- Once created and tested, evaluators can be saved and seamlessly integrated into workflows or prompt chains.

# Evaluator on demand

If you want us to build a specific evaluator for your needs, please drop a line at [contact@getmaxim.ai](mailto:contact@getmaxim.ai)

























