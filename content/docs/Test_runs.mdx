---
title: Runs
description: Runs are a vital component of the Maxim platform, enabling users to assess their applications across a spectrum of evaluators, including both Auto Evals and Human Evals. With Test Runs, users can execute evaluations and generate detailed reports for each run, facilitating thorough analysis and optimisation of their applications.
---
# Execution Process

<img src="./Test_runs/img1.png" alt="chameleon" />
<img src="./Test_runs/img2.png" alt="chameleon" />


Once you've configured your <a style={{color:"#FFA500"}} href="https://www.w3schools.com">`workflow`</a>, <a style={{color:"#FFA500"}} href="https://www.w3schools.com">`datasets`</a>, <a style={{color:"#FFA500"}} href="https://www.w3schools.com">`Context sources`</a> and <a style={{color:"#FFA500"}} href="https://www.w3schools.com">`evaluators`</a>, you can effortlessly test your workflow on the designated test dataset with a single click from the Workflows tabs. Alternatively, you can seamlessly integrate testing into your `CI/CD` pipelines for automated validation. These test runs provide performance insights at both the individual input-output combination level and collectively at the workflow + dataset level.
<Callout title="Title" type="info">
  You can also trigger test runs from `Promts` or `Prompt Chains`
</Callout>
# Monitoring and Reporting
<img src="./Test_runs/img3.png" alt="chameleon" />

Test Runs provides a user-friendly interface where you can access all executed `test runs`. Within this UI, you can obtain individual reports for each test run, containing comprehensive details such as the dataset used, start time, and current status of the run.
<img src="./Test_runs/img4.png" alt="chameleon" />

***Within the Test Runs interface:***
    - Users can access a detailed overview of every evaluator employed within each `workflow`.
    - This offers valuable insights into performance metrics and scores.
    
***Comprehensive analysis enables users to:***
 - Compare and evaluate the effectiveness of different evaluators across various workflows.
        - Facilitate informed decision-making and optimization strategies.
  
 ***With access to in-depth information on evaluator performance, users can:***
        - Identify strengths, weaknesses, and areas for improvement.
        - Ensure the continuous enhancement of their workflows.
# Monitoring Over time across different versions
<img src="./Test_runs/img5.png" alt="chameleon" />

Your team can analyse and compare these test runs across various configurations of their workflows, tracking progress trends over time amidst evolving `datasets` and `workflows`. This analysis provides development teams with valuable insights into the effectiveness of different queries, areas for improvement, and potential fallback mechanisms.

<Cards>
  <Card title="AI Evaluators" href="https://nextjs.org/docs" />
  <Card title="API-Based Evaluators" href="https://fumadocs.vercel.app" />
  
</Cards>