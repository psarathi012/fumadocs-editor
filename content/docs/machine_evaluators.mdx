---
title: Machine Evaluators
description: In Maxim, Programmatic Evaluators use code snippets to validate criteria like keyword presence or email correctness, while Statistical Evaluators analyze data patterns and metrics to assess dataset accuracy and reliability.
---
# Programmatic Evaluators
 In Maxim, you can construct Programmatic Evaluators akin to how API-based evaluators are created. These evaluators are code snippets designed to validate specific criteria, such as checking for a particular keyword or verifying a correct email address. 

```js title="config.js"
function validate(input, output, expectedOutput) {
    const specialCharRegex = /[!@#$%^&*(),.?":{}|<>]/;
    return specialCharRegex.test(output);
}
```
This function named <span style={{color:"#FFA500"}}>validate</span>
 takes three parameters: <span style={{color:"#FFA500"}}>input</span>, <span style={{color:"#FFA500"}}>output</span>, and <span style={{color:"#FFA500"}}>expectedoutput</span>. It aims to check if the output contains any special characters `(such as !@#$%^&*(),.?":{}|<>)`. It uses a regular expression `(specialCharRegex)` to match any of these special characters against the output. If any special character is found in the output, the function returns true, indicating that the output is considered valid.

<img src="./img/img1.png" alt="chameleon" />

- Programmatic evaluators, akin to API-based ones, allow defining pass/fail criteria.
- They enable direct testing of functions.
- For instance, if ```{run}``` is passed as output to the `validate` function, it returns true due to special characters ```"{"``` and ```"}"```.
- Once created and tested, evaluators can be saved and seamlessly integrated into workflows or prompt chains.

# Statistical evaluators 
While Maxim includes statistical evaluators like <span style={{color:"#FFA500"}}>BLEU</span> and <span style={{color:"#FFA500"}}>ROUGE</span> etc, users are unable to create their own statistical evaluators.
<img src="./img/img2.png" alt="chameleon" />

Assess the accuracy of generated output by comparing it to the expected result. BLEU, a metric for text evaluation, gauges the quality of output by comparing it to reference texts. It scores based on shared n-grams between machine-generated and reference texts, with higher scores indicating better alignment and therefore better output quality.

<Cards>
  <Card title="AI Evaluators" href="https://nextjs.org/docs" />
  <Card title="API-Based Evaluators" href="https://fumadocs.vercel.app" />
  
</Cards>

