---
title: View test runs
description: Test Runs is the corner stone of the Maxim platform, enabling you to assess all the evaluation report of your application. With Test Runs, you can access your previous executions of runs and view the historic reports for each run, facilitating thorough analysis of your applications.
---

## Test  run reports
<img src="./Evaluate/test-runs/test-runs-view/img3.png" alt="Test report" />

Test Runs provides a user-friendly interface where you can access all executed test runs. Within this UI, you can obtain individual reports for each test run, containing comprehensive details such as the dataset used, start timestamp, and current status of the run etc.

<img src="./Evaluate/test-runs/test-runs-view/img4.png" alt="Test report" />

***Within the Test Runs interface:***
    - Users can access a detailed overview of every evaluator employed within each workflow.
    - This offers valuable insights into performance metrics and scores.
    
***Comprehensive analysis enables users to:***
        - Compare and evaluate the effectiveness of different evaluators across various workflows.
        - Facilitate informed decision-making and optimization strategies for improving your application.
        
***With access to in-depth information on evaluator performance at each query level you can:***
        - Identify strengths, weaknesses, and areas for improvement.
        - Ensure the continuous enhancement of your AI applications.
        - Investigate each entry for their respective scores and the reasoning

<LoomVideo url="https://drive.google.com/file/d/1TVEqpjITHXFwrDWwBRQbbNzLld70N5AS/preview" />



# 


# Monitoring your application quality across versions

<img src="./Evaluate/test-runs/test-runs-view/img5.png" alt="Test report" />

Your team can analyse and compare these test runs across different versions of their application, tracking progress over time amidst evolving datasets and ever changing moving parts of their compound AI system. This analysis provides development teams with valuable insights into the effectiveness of different prompts, different models and figure out areas for improvement, and potential fallback mechanisms if needed.

<Cards>
  <Card title="Overview" href="https://nextjs.org/docs" />
  <Card title="Share test run report" href="https://fumadocs.vercel.app" />
  
</Cards>