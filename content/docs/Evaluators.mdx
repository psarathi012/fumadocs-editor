---
title: Evaluators
description: Maxim provides an interface that seamlessly integrates human and machine evaluations for development teams building applications with generative AI.
---

For <a style={{color:"#FFA500"}} href="https://www.w3schools.com">`human evaluation`</a>, Maxim allows you to define the evaluation focus (e.g., factuality, naturalness) and description based on your application’s requirements. Once defined, you can get your workflows graded on these criteria by sharing them with any manual `reviewers` on your teams and outside.

For machine evaluations, you have access to a comprehensive set of built-in and custom evaluators across modalities. Which can be further broken down into:
- **AI Evaluators** -Built-in evaluators are a set of models that grade your workflow’s outputs on pre-defined criteria including `factuality`, `consistency`, `toxicity`, `relevance`, etc.
- **Statistical Evaluators**- assess text generation quality by comparing the generated text to reference texts using metrics like `BLEU` (measuring n-gram precision), and `ROUGE` (measuring n-gram recall and overlap).
- **Programmatic Evaluators**-  assesses and validates specific conditions within text or data, performing tasks like detecting special characters, validating emails, and counting word occurrences to ensure accuracy and efficiency in data processing and analysis. You can also write your own custom Programmatic Evaluator
- **API Based Evaluators**- Many companies have developed their own fine-tuned models for `semantic similarity` or `consistency`, etc, so you can integrate your own evaluator endpoint into Maxim for evaluation.


<Cards>
  <Card title="AI evaluators" href="https://nextjs.org/docs" />
  <Card title="Statistical Evaluators" href="https://fumadocs.vercel.app" />
  <Card title="Programmatic Evaluator" href="https://fumadocs.vercel.app" />
  <Card title="API Based Evaluators" href="https://fumadocs.vercel.app" />

</Cards>