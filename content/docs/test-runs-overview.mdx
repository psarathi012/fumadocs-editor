---
title: Overview
---

# Test and evaluate prompts and workflows
After configuring your prompts/workflows, datasets, and evaluators in Maxim, you can easily test your prompts or workflow on the test dataset and the selected evaluators. This allows you to evaluate your application seamlessly in just a few clicks. All the historical test runs are stored in the run tab under 

## Analysing test runs
1. **Comparison Across Configurations**: Compare all your historical test runs across different prompt and workflow configurations. This feature allows you to track progress and trends over time as your datasets and workflows evolve.
2. **Insight into Queries**: Gain valuable insights into which queries are performing well and which are not. This helps in pinpointing areas that need enhancement and identifying the right fallback mechanisms.
3. **Performance Trends**: Observe how the performance of your workflows changes over time with changes in prompts, or with respect to a new model or against your evolving datasets. This helps in iterating and improving the efficiency and accuracy of your AI applications.

## Share reports
**Simple Sharing:**
Share the test run reports easily with anyone via a simple share link. This feature facilitates collaboration and communication within your team or with external stakeholders. This report is visible to anyone who you share with and they donâ€™t need to be on the platform to view the report.

<Cards>
  <Card title="View test runs" href="https://nextjs.org/docs" />
  <Card title="Share test run report" href="https://fumadocs.vercel.app" />
  
</Cards>